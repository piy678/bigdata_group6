








import os
import sys

from pyspark.sql import SparkSession

os.environ["JAVA_HOME"] = "C:\\Program Files\\Eclipse Adoptium\\jdk-11.0.27.6-hotspot"


os.environ["PYSPARK_PYTHON"] = sys.executable







 # TODO
!pip install kafka-python
!pip install confluent-kafka
!pip install pyspark







# TODO
import pandas as pd
import json

# CSV laden
df = pd.read_csv(
    "C:/Users/pinar/Downloads/kafka_spark_streaming/kafka spark streaming/notebooks/Top_100_most_Streamed.csv")

# Optionale Umbenennung: Punkt in Spaltennamen durch Unterstrich ersetzen
df.columns = [col.replace('.', '_') for col in df.columns]

# Optional: String-Konvertierung (z. B. bei Jahr oder Länge)
df["year"] = df["year"].astype(str)
df["length"] = df["length"].astype(str)  # falls z. B. '200' Minuten ist

# JSON-Nachrichten generieren
json_messages = df.to_dict(orient="records")






# TODO
import pandas as pd
import json
from kafka import KafkaProducer
import time

# 1. CSV laden
df = pd.read_csv(
    "C:/Users/pinar/Downloads/kafka_spark_streaming/kafka spark streaming/notebooks/Top_100_most_Streamed.csv")

# 2. Spaltennamen bereinigen
df.columns = df.columns.str.strip().str.lower().str.replace(".", "_").str.replace(" ", "_")

# 3. Nachrichten vorbereiten
json_messages = df.to_dict(orient="records")

# 4. Kafka Producer
producer = KafkaProducer(bootstrap_servers='localhost:9092')

# 5. Senden
for msg in json_messages:
    message = json.dumps(msg).encode("utf-8")
    producer.send("music_data", message)
    print(f"Gesendet: {msg.get('title', '??')} von {msg.get('artist', '??')}")
    time.sleep(0.5)

producer.flush()
print("Alle Songs erfolgreich gesendet.")









# TODO
from kafka import KafkaConsumer
import json

# Consumer erstellen
consumer = KafkaConsumer(
    'music_data',  # Topic-Name
    bootstrap_servers='localhost:9092',
    auto_offset_reset='earliest',  # liest von Anfang
    enable_auto_commit=True,
    group_id='music_consumer_group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
        consumer_timeout_ms=60000
)
import time
start_time = time.time()
max_duration = 1 * 60
# Nachrichten lesen und anzeigen
print("Starte Kafka Consumer...\n")

for message in consumer:
    song = message.value
    print(f"Empfangen: {song.get('title', '??')}")
    print(f"von {song.get('artist', '??')}")

    if time.time() - start_time > max_duration:
        print("\n1 Minuten erreicht. Consumer wird beendet.")
        break

consumer.close()









from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("KafkaExample") \
    .config("spark.jars", ",".join([
        "C:/Users/pinar/Downloads/spark/jars/spark-sql-kafka-0-10_2.12-3.3.0.jar",
        "C:/Users/pinar/Downloads/spark/jars/spark-token-provider-kafka-0-10_2.12-3.3.0.jar",
        "C:/Users/pinar/Downloads/spark/jars/kafka-clients-2.8.0.jar"
    ])) \
    .getOrCreate()







from kafka import KafkaConsumer
import json
from pyspark.sql import SparkSession, Row
spark = SparkSession.builder \
    .appName("KafkaConsumerToSpark") \
    .master("local[*]") \
    .getOrCreate()

# Kafka-Consumer: Nachrichten aus dem Topic lesen
consumer = KafkaConsumer(
    'music_data',
    bootstrap_servers='localhost:9092',
    auto_offset_reset='earliest',
    enable_auto_commit=True,
    value_deserializer=lambda m: json.loads(m.decode('utf-8')),
    consumer_timeout_ms=2000  # Warten auf neue Nachrichten
)

messages = [msg.value for msg in consumer if isinstance(msg.value, dict)]
consumer.close()

for i, msg in enumerate(messages):
    if not isinstance(msg, dict):
        print(f"[{i}] Nicht dict: {msg}")
    else:
        print(f"[{i}] OK: {msg}")

# Nachrichten in Spark-DataFrame umwandeln
import json
from pyspark.sql import Row

rows = []
for i, msg in enumerate(messages):
    try:
        # Schritt 1: JSON-String → Dict
        parsed = json.loads(msg) if isinstance(msg, str) else msg
        # Schritt 2: Nur wenn Dict → Row
        if isinstance(parsed, dict):
            rows.append(Row(**parsed))
    except Exception as e:
        print(f"[{i}] Fehler bei Nachricht: {msg}\n{e}")

df = spark.createDataFrame(rows)

# Anzeigen
df.show()
df.printSchema()






from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# SparkSession erstellen
spark = SparkSession.builder \
    .appName("KafkaExample") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .getOrCreate()

# Kafka-Stream lesen
df_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "music_data") \
    .load()

# Kafka-Binary-Value in String umwandeln
df_strings = df_raw.select(col("value").cast("string").alias("value_str"))

# Stream in Konsole ausgeben
df_strings.writeStream \
    .outputMode("append") \
    .format("console") \
    .start() \
    .awaitTermination()






# TODO
from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType

schema = StructType([
    StructField("id", IntegerType()),
    StructField("name", StringType()),
    StructField("duration", IntegerType()),
    StructField("explicit", BooleanType())
])
from pyspark.sql.functions import col, from_json
from pyspark.sql import SparkSession


spark = SparkSession.builder \
    .appName("KafkaTest") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1") \
    .getOrCreate()

df_kafka_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "dein-topic") \
    .load()

df_parsed = df_kafka_raw.selectExpr("CAST(value AS STRING) as json_str") \
    .select(from_json(col("json_str"), schema).alias("data")) \
    .select("data.*")






# TODO
from pyspark.sql.functions import from_json, col, window
from pyspark.sql.types import StructType, StringType, TimestampType

# Schema definieren
schema = StructType() \
    .add("message", StringType()) \
    .add("type", StringType()) \
    .add("timestamp", TimestampType())

# Kafka-Daten in String umwandeln und parsen
df_parsed = df_raw.selectExpr("CAST(value AS STRING)") \
    .select(from_json(col("value"), schema).alias("data")) \
    .select("data.*")

# Gruppieren nach Zeitfenster
df_windowed = df_parsed \
    .groupBy(window(col("timestamp"), "1 minute"), col("type")) \
    .count()

# Ausgabe im Console-Sink
df_windowed.writeStream \
    .outputMode("complete") \
    .format("console") \
    .option("truncate", False) \
    .start() \
    .awaitTermination()






# TODO
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import StructType, StructField, StringType, TimestampType
from pyspark.sql.functions import from_json

# Spark-Session erstellen
spark = SparkSession.builder \
    .appName("KafkaStructuredStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0") \
    .getOrCreate()

# Kafka-Stream lesen
df_raw = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "your_topic_name") \
    .load()

# Kafka-Wert (binary) zu String konvertieren
df_strings = df_raw.selectExpr("CAST(value AS STRING)")

# Schema definieren (falls JSON-Nachrichten erwartet werden)
schema = StructType([
    StructField("message", StringType()),
    StructField("timestamp", TimestampType())
])

# JSON-Daten parsen (optional, falls value JSON ist)
df_parsed = df_strings.select(from_json(col("value"), schema).alias("data")).select("data.*")

# Streaming-Ergebnis in Memory schreiben (queryName ist nötig für memory sink)
query = df_parsed.writeStream \
    .format("memory") \
    .outputMode("append") \
    .queryName("messages_table") \
    .start()

# Verarbeitung laufen lassen
query.awaitTermination()






# TODO
import pandas as pd
import matplotlib.pyplot as plt

# Lies Daten aus der Spark Memory Table (die vorherige writeStream mit queryName="messages_table" muss laufen!)
df_spark = spark.sql("SELECT * FROM messages_table")

# In Pandas konvertieren
df_pandas = df_spark.toPandas()

# Zeige die ersten Zeilen zur Kontrolle
print(df_pandas.head())

# Beispielhafte Visualisierung (z. B. Nachrichtenanzahl nach Zeit, falls 'timestamp' vorhanden ist)
if 'timestamp' in df_pandas.columns:
    df_pandas['timestamp'] = pd.to_datetime(df_pandas['timestamp'])
    df_grouped = df_pandas.groupby(pd.Grouper(key='timestamp', freq='1Min')).size()

    # Plotten
    plt.figure(figsize=(10, 5))
    df_grouped.plot(kind='bar')
    plt.title('Anzahl der Nachrichten pro Minute')
    plt.xlabel('Zeit')
    plt.ylabel('Nachrichten')
    plt.tight_layout()
    plt.show()
else:
    print("Spalte 'timestamp' nicht vorhanden – einfache Visualisierung nicht möglich.")

